{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04118e10-8c7e-4055-aea6-fdfd61dd5eac",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66716f-5a3a-41b9-b35d-4988af67856a",
   "metadata": {},
   "source": [
    "It is a generic optimatization algorithm which is capable of finding the optimal soultion to a wide range of problems. \n",
    "The general idea of a Gradient Descent is to tweak the parameters iteratively in order to minimize a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac435b-701d-4264-8d20-51372c379d43",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b0bdb-a00d-4548-94e3-655cd1776e81",
   "metadata": {},
   "source": [
    "## Preparing the data which can be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ed5ed3-3220-433e-8eb8-d74ec0c04cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Importing Library\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1) \n",
    "X_new = np.array([[0], [2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b2371d-6ac8-44ae-9dd8-b332c5a93645",
   "metadata": {},
   "source": [
    "Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4424bf9-baa0-4863-b5d8-a6fabb5f6584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  # Importing Library\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$X$\", fontsize=15)\n",
    "plt.ylabel(\"$Y$\", rotation=0, fontsize=15)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a80d01-ad5e-48ba-981c-668c45cc8ee6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57772d8-a725-4334-9dd6-f7d9864652d0",
   "metadata": {},
   "source": [
    "## Applying Linear Regression as a base standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4caccb95-2225-4d71-a55e-835c709a3053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [4.50560009]\n",
      "Coefficient: [[2.99583004]]\n",
      "Prediction\n",
      "[[ 4.50560009]\n",
      " [10.49726017]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "print(f'Intercept: {lin_reg.intercept_}')\n",
    "print(f'Coefficient: {lin_reg.coef_}')\n",
    "\n",
    "print('Prediction')\n",
    "print(lin_reg.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e6f1f-8182-49e6-af75-f2f243466009",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faaf753-7680-4b37-869c-433ced53f3d8",
   "metadata": {},
   "source": [
    "## General Process of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f7bc1-b1b4-4c3d-9741-6e163cc5fbc6",
   "metadata": {},
   "source": [
    "- We start at a random value, this is called **Random Initialization**\n",
    "- Then we calculate the cost function (eg. MSE) for that value\n",
    "- We then move slowly to the next value where the cost function is decreasing\n",
    "    - The steps by which we move is determined by the **Learning Rate** hyperparameter\n",
    "- Repeating the above steps will eventually result in the most optimized value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ecef1-ac1c-4a57-bba3-65b8570ca6c4",
   "metadata": {},
   "source": [
    "## Things to remember about Gradient Descent Algorithm\n",
    "- Make sure all the features have a similar scale or else it will take much longer to converge on to the optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c7ae7-0848-49f2-b7c7-a6ac93083fda",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621b5dca-df1a-41d2-a0ce-42a07eb8b482",
   "metadata": {},
   "source": [
    "## Types of Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee30b0-dfd5-4aa7-a01f-29cbb22fb90f",
   "metadata": {},
   "source": [
    "- Batch Gradient Descent\n",
    "- Stochstic Gradient Descent\n",
    "- Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30676bb-cd7c-468a-9d81-bd0547c2e995",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent:\n",
    "- It uses the whole batch of training data at every step\n",
    "- As a result it is very slow on large training sets\n",
    "- But it scales very well with the number of features\n",
    "- Comparing a Linear Regression model where there are hundreds of features, Batch Gradient Descent is much faster than the Normal Equation or the SVD decomposition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b929f65-cb74-4a07-8e37-562e60ed2657",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent:\n",
    "- Instead of using the entire batch of training data every time, this picks a random instance in a training set and computes the gradients based only on that single instance\n",
    "- As a result, this is much faster and it also makes it possible to train on huge training sets since only one instance needs to be in the memory\n",
    "- Due to its stochastic nature, the algorithm is much less regular; instead of gently decreasing, the cost function keeps bouncing and decreasing only on average\n",
    "- Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around and never settle down. So once the algorithm stops, the solution is good but not optimal\n",
    "- The randomness is good to escape any local minimum, but bad since it means that the algorithm will never settle at the minimum\n",
    "- Solution to the above problem, is to reduce the learning rate of the model so that in the beginning it's high enough to jump out of local minimum but as it progress the learning rate is reduced to reach the optimal solution\n",
    "- The function that determines the learning rate at each iteration is called *Learnign Schedule*\n",
    "- If the learning rate is reduced too quickly then you may end up on a sub-optimal solution, whereas if the learning rate is reduced too slowly then you will end up jumping around the optimal solution for quite a while\n",
    "- Another approach is to run the code until the loss drop of the cost function is not less than a certain value called *Tolerance*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d9cd9-6703-4886-b431-a2cc17328dba",
   "metadata": {},
   "source": [
    "#### Performing Linear Regression using Stochastic Gradient Descent approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b55652-c2b8-4a72-b2a8-e14bb5fe6e53",
   "metadata": {},
   "source": [
    "The following code applies Stochastic Gradient Regressor, which runs for a maximum of 1000 epochs *(iterations)*, or untill the loss drops by less than 0.001 during one epoch *(tolerance)*. It starts with a learning rate of 0.1 *(eta0)*, using the default learning schedule. It doesn't use any type of regularization *(penalty)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a53fffc-6a4e-49de-9e5d-183c13f1b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [4.49888819]\n",
      "Coefficient: [3.00451325]\n",
      "\n",
      "Predictions\n",
      "[ 4.49888819 10.5079147 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000,\n",
    "                      tol=1e-3,\n",
    "                      eta0=0.1,\n",
    "                      penalty=\"none\")\n",
    "sgd_reg.fit(X, y.ravel())  #ravel returns a 1D array\n",
    "print(f'Intercept: {sgd_reg.intercept_}')\n",
    "print(f'Coefficient: {sgd_reg.coef_}')\n",
    "\n",
    "print('\\nPredictions')\n",
    "print(sgd_reg.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e36917-d88b-446f-94fa-af850711a526",
   "metadata": {},
   "source": [
    "As we can see that the solution is quite close to the Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba54e1-9588-448b-8ced-02b9079fbe52",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68084c-998d-40df-8ae6-ed2775da3db8",
   "metadata": {},
   "source": [
    "- It combines both Batch and Stochastic Gradient Descent algorithms\n",
    "- At each step, instead of comuting the gradients based on a single instance of the training set *(Stochastic Gradient Descent)* or based on the full training set *(Batch Gradient Descent)*, it computes based on a small random set of the training set called *Mini Batches*\n",
    "- Main advantage:\n",
    "    - Performance gain over the Batch Gradient Descent algorithm\n",
    "    - Less eratic than the Stochastic Gradient Descent algorithm\n",
    "- It will get closer to the optimal solution, but it may be harder for it to escape the local minimum due to less eratic behavior "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d774969-b400-4c1b-8044-c2f46e598412",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df74b2f-2f2c-4786-87be-1fc394d4c901",
   "metadata": {},
   "source": [
    "## Comparisor of algorithms for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a3aa0-13a2-4063-bf16-f35cb60c8dd4",
   "metadata": {},
   "source": [
    "|Algorithm|Large *m*|Out-of-core support|Large *n*|Hyperparameters|Scalling required|Scikit-Learn|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Normal Equation|Fast|No|Slow|0|No|N/A|\n",
    "|SVD|Fast|No|Slow|0|No|`LinearRegression`|\n",
    "|Batch GD|Slow|No|Fast|2|Yes|`SGDRegressor`|\n",
    "|Stochastic GD|Fast|Yes|Fast|>=2|Yes|`SGDRegressor`|\n",
    "|Mini-batch GD|Fast|Yes|Fast|>=2|Yes|`SGDRegressor`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e39cf-f2b2-434e-9502-a178bf117b4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a33810-dc85-45ad-8d32-e97bb7eb2e24",
   "metadata": {},
   "source": [
    "## General Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3b128-fce4-4ce6-bdb8-163b4df6c629",
   "metadata": {},
   "source": [
    "- **Random Initialization**\n",
    "    - Starting the Gradient Descent Algorithm at a random initial value\n",
    "- **Learning Rate**\n",
    "    - Size of the step by which the algorithm moves to the next step of its evaluation\n",
    "    - If the learning rate is too low, then it will take too long for the algorithm to converge on the most optimal souliton\n",
    "    - If the learning rate is too high, then the algorithm will bounce around and not settle on the most optimal solution\n",
    "    - In order to find a good learning rate, we can use `Grid Search`\n",
    "- **Local Minimum**\n",
    "    - In cases where the cost function isn't a regular bowl shaped, there could be a point which is the most optimal solution since there could be some sort of holes, ridges, plateaus or irregular terrain\n",
    "- **Global Minimum**\n",
    "    - This is the most optimal solution in case a local minimum is present\n",
    "- **Convex Function**\n",
    "    - A function where if you pick any two points on the curve, the line joining them never crosses the curve. This imples that there are no local minimum, just one global minimum\n",
    "    - MSE function of a Linear Regression is a convex function, so a gradient descent approach on a linear regression is gaunranteed to reach the global minimum\n",
    "- **Epoch**\n",
    "    - Iterations of the model\n",
    "- **Learning Schedule**\n",
    "    - The function that determines the learnign rate of the model\n",
    "    - If the learning rate is reduced too quickly then we may end up on a sub-optimal solution\n",
    "    - If the learning rate is reduced too slowly then we will end up jumping around the optimal solution for too long\n",
    "- **Tolerance**\n",
    "    - The threshold of the loss drop for a cost function during one epoch which determines if we need to stop the code\n",
    "    - Useful in cases where we are not sure about how long we need to run a particular model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae06db-8250-4ae7-81d5-5a2adf732f83",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb6d4f-3e78-4b8b-bc92-667eceac87b0",
   "metadata": {},
   "source": [
    "## Credits\n",
    "- Complete credit goes to Hands-on Machine Learning  with  Scikit-Learn, Keras & TensorFlow book by Aurélien Géron\n",
    "- This notebook is my understanding of the topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
